---
title: "Image Exercise"
author: "Dasapta Erwin Irawan, Willem Vervoort and Gene Melzack"
date: "Today's date is `r format(Sys.Date(),'%d-%m-%Y')`"
output: 
    pdf_document:
      fig_width: 7
      fig_height: 6
      fig_caption: true
---

```{r setup, warning=F, message=F, echo=F}
# root dir
knitr::opts_knit$set(root.dir =  "c:/users/rver4657/owncloud/working/SSEAC/opendataworkshop2017/exercise")

```

## Introduction

To demonstrate the overall process of collecting data, creating reproducible research, and publishing data  we have designed an exercise that will allow you to plot the results of the vegetation images that we have collected during our small field excursion. While the current data are arbritrary, they contain several aspects that are useful to demonstrate steps in the open research data process.
R markdown is a nice way to actually present your reproducible research, as it allows you to record both the code and the text.

In this exercise, we will do three separate components:  

1. Learn how to plot spatial data and convert from latitude and longitude to eastings and northings  
2. Understand how to extract the different colour channels from photos and calculate GCC following the paper by [Moore et al (2017)](https://drive.google.com/open?id=1K98yqDwf7P186wZITbM7sCTamo0Doh52)  
3. Plot the GCC values in space to make a map of the collected data.

Here we will use some example data and do some exercises, after which we can apply this to the field data we will collect on Thursday.

## Plotting spatial data

Spatial data is generally in latitudes and longitudes. This can be both in decimal form, such as in Google maps, or in degree form (which is much of the older data). As latitudes and longitudes are related to the globe, to calculate actual distances and to display correct spatial relationships, the data needs to be projected (or flattened). For projections we need to use the right coordinate system. If anyone of you has worked with GIS, then this is old news. 

Here we first use some example data from Australia:

* This file AusLatLongData.txt is a simple comma delimited text file that has decimal latitudes and longitudes in and around Sydney.

We first need to load the data and we can assign a spatial coordinate system to it that indicates that it is in latitude and longitudes. We can then plot the data in these coordinates. 

We also need a few libraries: rgdal, sp

```{r AusLLData}
# library
library(tidyverse)
library(rgdal)
library(sp)

# root dir
root <- "C:/Users/rver4657/Google Drive/ITB_Usyd_Project2017/workshop_opendata/OriginalDataFolder"
# read in the data
Sydney <- read.csv(paste(root,"AusLatLongData.txt",sep="/"))

# create Spatial points data frame using package sp and insert coordinate reference
coord_ref <- CRS("+proj=longlat +datum=WGS84")
Sydney_sp <- SpatialPoints(coords=Sydney[,2:3], proj4string = coord_ref)
# we can plot this
p <- ggplot(aes(x = Longitude, y = Latitude), data = as.data.frame(Sydney_sp)) + geom_point(col="red", size=4) +
  geom_text(aes(label = Sydney[,1]))
print(p)

```

However, this does not reflect the real distances, for this we would like to convert the latitudes and longitudes into eastings and northings. This is a projection and we need to use the correct projection for the Sydney area.
For this we simply use the correct epsg code using [\textcolor{blue}{the spatial reference system}](http://spatialreference.org), which for Sydney is zone 56 or epsg code [\textcolor{blue}{28356}](http://spatialreference.org/ref/epsg/28356/).

We can now simply calculate the distance between Abbotsford and the airport in m.

```{r projection}
# reproject
Sydney_pr <- spTransform(Sydney_sp, CRS("+init=epsg:28356")) # reproject

# plot again
p <- ggplot(aes(x = Longitude, y = Latitude), data = as.data.frame(Sydney_pr)) + geom_point(col="red", size=4) +
  geom_text(aes(label = Sydney[,1])) +
  xlab("Easting") + ylab("Southing")
print(p)


# distance Abbotsford (2nd row) to Airport (3rd row)
Dist <- sqrt((Sydney_pr@coords[2,1]-Sydney_pr@coords[3,1])^2 + 
            (Sydney_pr@coords[2,2]-Sydney_pr@coords[3,2])^2)
paste("the distance between Abbotsford and Sydney airpoirt =",round(Dist,2), "m")

```

## Exercise

- Repeat the above analysis using the Bandung data that is also on the drive. Note that this data does not have any named labels, just row numbers.

## Image analysis
In the field excursion we collected some images, which we downloaded into a shared directory. Here we will do a simple analysis to extract the red and blue bands and calulate the average ratio of the red and blue bands, because we don't actually have access to any Near-Infrared to mimic NDVI.

The first thing we need to do is to install the package `imager`, or just require if you have this already installed. We will also use the package `tidyverse` which we have seen before.

```{r imager, message=F, warning=F}
# not installed? run
# install.packages("imager")
# otherwise
library(imager)
library(tidyverse)

```

The next step is to load an image and to convert this to a data.frame

```{r loadImage, fig.cap = "A demonstration picture"}
our_image <- load.image("imageanalysis/5.jpg")
plot(our_image)
image_df <- as.data.frame(our_image)
head(image_df,3)
```

As we can see, the data consists of the pixel x and y coordinate and a numerical value to indicate the R (red), G (green), and B (blue) channel. The data.frame is *stacked*. We can quickly plot the distributions of the different channels using `ggplot()`, and splitting the plot by channel.
The first line of code assigns 'R', 'G' and 'B' to the numerical values for the channels. The next lines of code creates the histograms.

```{r plotImage, fig.cap = "histograms of the different channels"}
image_df <- mutate(image_df,channel=factor(cc,labels=c('R','G','B')))
ggplot(image_df,aes(value,fill=channel))+
  geom_histogram(bins=30)+
  facet_wrap(~ channel)
```

## Creating the average R over R + B ratio

The next step is to calculate the R/(R + B) values, which as outlined represent vegetation growth (Moore et al. 2017). For this we first need to *spread* the data.frame to create separate columns for R, G and B, and in the mean time dropping the column cc using `mutate()`.  

```{r spread}
image_df2 <- image_df %>% 
  mutate(cc = NULL) %>%
  spread(key = channel, value = value)
head(image_df2)
```

Now it is simple to calculate the ratio using `mutate()` once again, plot a simple histogram of this data and finally calculate the mean value for the image.

```{r ratio, fig.cap = "histogram of the R/(R + B) ratio"}
image_df3 <- image_df2 %>%
  mutate(ratio = R/(R+B))

pl <- ggplot(image_df3,aes(ratio)) +
  geom_histogram(bins=50) + xlab("R/(R + B) ratio")
print(pl)

# calculate the mean
avg_ratio <- mean(image_df3$ratio, na.rm=T)
round(avg_ratio,2)
```

## extracting the latitude and longitude data and plotting

A nice feature of modern digital photography is that it directly comes with a lot of metadata including the latitude and longitude of where the photo was taken, the metadata on the images is called the **exif data**. There is a package in R that does this quite nicely, which is called `exifr`. There is additional package that allows plotting onto google map images. This is called `leaflet`.

There is one small problem with `exifr` on Windows. The package assumes that you have the **perl** computer scripting language (similar to R) installed on your computer. This is the case on all Apple computers, but not on Windows. So on windows you have to first install Perl from:[http://strawberryperl.com/](http://strawberryperl.com/). 
Then you can installl `leaflet` and `exifr`.

```{r install_exifr}
#install.packages(c("exifr", "leaflet"))
# you have to install Perl on windows, as this is not included, it is included on Apple
# then on windows point exifr to perl dir
#options(exifr.perlpath='c:/strawberry')
# then load the libraries into your workspace
library(exifr)
library(leaflet)
```

We can now read all the jpg files from a directory, and extract the information

```{r plotjpglocations}
files <- list.files(path = "imageanalysis/vegetation_images", 
                    pattern = "*.jpg")
dat <- read_exif(path = paste("imageanalysis/vegetation_images/",
                              files,sep=""))
names(dat)

```
There is a lot of metadata with each photo and we can explore all of this, but we are only interested in the Latitude and Longitude. First we can make a simple plot of Longitude and Latitude.

```{r simpleplot, fig.cap = "plotting the latitude and longitude of the metadata"}
plot(dat$GPSLongitude, dat$GPSLatitude)

```

## Calculate GCC values and plot

The final step is to calculate the GCC values for all the images and plot these on the map. We will do this by running a loop over files, and reusing the code from above inside the loop (but dropping all the plotting and looking at the files)

```{r plotGCC}
Store <- rep(0,length(files))

for (i in 1:length(files)) {
  our_image <- load.image(paste("imageanalysis/vegetation_images/",
                              files[i],sep=""))

  image_df <- as.data.frame(our_image)
  image_df <- mutate(image_df,channel=factor(cc,labels=c('R','G','B')))
  # spread
  image_df2 <- image_df %>% 
      mutate(cc = NULL) %>%
      spread(key = channel, value = value)
  # mutate
  image_df3 <- image_df2 %>%
    mutate(ratio = R/(R+B))
  # calculate the mean
  avg_ratio <- mean(image_df3$ratio, na.rm=T)
  # Store
  Store[i] <-round(avg_ratio,2)
}

plotdat <- data.frame(Latitude = dat$GPSLatitude,
                      Longitude = dat$GPSLongitude,
                      GCC <- Store)


# now plot values of Store
p <- ggplot(plotdat, aes(x=Longitude, y = Latitude)) +
  geom_point(aes(colour=GCC, size=GCC))
p
```