---
title: "Intro to R"
author: "Willem Vervoort and Dasapta Erwin Irawan" 
date: "09 December 2017"
output: 
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 2
---

# Introduction  

This is an introduction to R written for the "Open data workshop in Bandung 5-9 Feb 2018", jointly organised by the University of Sydney and Institut Teknologi Bandung. 

This work is based on earlier events from the authors and it also builds on many of the introduction to R literature on the internet.    

This course is not a complete introduction, and more in depth knowledge on R and the use of R can be gained from many courses on-line and by basic practice.  

This course covers simple R, basic statistics, data frame operations, reading in files and a plotting. 

We hope that this course offers sufficient depth to help you engage in the rest of the course, which uses R for analysis of the output.


\newpage

# R as a modelling environment

The origins of R are in statistics, so this is hat R does best. However, over time, it has proven to be a flexible language that can also be used quite effectively for programming and data science.

## R and R Studio

### Base R vs IDE

If R is the machine under the hood, then R Studio would be the dashboard, steering wheel, as well as the gas and brake paddles. People frequently mention R as `base R` and R Studio is an Integrated Development Environment (IDE). 

Is there another IDE other than R Studio? The answer is Yes. You could check out R Commander, Revolution R. 

### Running R online 

Can we run R online? The answer is also Yes. R Studio offers a paid cloud service. You could try [R fiddle](http://www.r-fiddle.org/) for a limited range of code of package installation, [CoCalc/Sage Math Cloud](https://cocalc.com/), [Jupyter](https://jupyter.org/), and the New Comer [Code Ocean](https://codeocean.com/).   

### R is cross platform

R and R Studio are cross platform. So you could use R on these major OS', Windows, Mac or Linux, so it's OK if you work with another person who don't use the same OS as you do. You just have to make sure that all parties have the same data and the same package installed in the system, and the same codes to run.

### How to install R and R Studio

We recommend to install R first followed by R Studio. Install R from [CRAN](http://cran.at.r-project.org/) and R Studio from its official [site](http://rstudio.com).

### R components

In R programming, as also in other programming language, the two main components are the data and the codes. Using both, you could start an analysis and produce plots and tables as outputs. So in order to do the analysis, we will need `package`. 

The good thing about R is, there are `base commands`, that is commands that included in the base R installation. This commands are progressing as you install newer version of R. It's getting better and easier through time. But users are allowed to make their own commands that consists of a function or sets of functions. Sets of function can be grouped as a `package`. So you would need to install the package first and load the package, before using the command or function inside that package. You would only need to install the package once.

You could run this line to install a package from CRAN server.
```
install.packages("packageName") # case sensitive
library(packageName) # to load the package
```
Other than CRAN, you may find packages that are still in development stage on [GitHub](http://github.com), a repository of codes. You could install a package on GitHub using `install_github` command from `devtools` package.

```
install.packages("devtools")
library(devtools)
install_github(repoAddress)
```

Now you could run a command from a package without loading the package. But off course you still need to install the package in to your system

```
install.packages("devtools")
devtools::install_github(repoAddress)
```

### Navigation

In R Studio, you would see four panels (clock-wise): Codes on top right, Environment, Files/folder/plots/packages, and console. You write your lines of code in the code panel then observe the process of you code in the console panel. Find out in console, if your codes are running well or have a problem (error messages), or just a warning. Then you could see all components that related to the process in the Environment panel.

![Four panels in R Studio](four_panels.png)

### Working folder structure
In R and in any other command line-based application, you would need to tell the app your current folder location and the location of the data. Usually we use the following folder structure:

 - main project folder
  - data: put your data here
  - code: put your code here
  - output: put your plots and tables here
  - text: put your report here

But in practice, we usually work with code, data, outputs in one folder, but use it as a process or intermediate folder. We usually sort out the components at the final stage of our work. But when is "final stage" be? So do the file sorting several times.

#### Exercise

- Can you check your working folder/directory and what's inside it?

\newpage

# BASIC R  

## R as a calculator

In its most basic form, R is a calculator 
```{r calculator}
3*5
50/100 + 0.1
10 - 20
```

## Objects in R

The basic structure of R is based on objects, which are named. R is case sensitive, so keep this in mind. The main object we will use here is a *dataframe* or its modern variant the *tibble*.  

All objects will be loaded on to the memory. So if you have a datafile, the first thing to do is to load it on your memory as an object that can be seen in the Environment panel. Thus, whatever you do with the object will not change your file, unless you save the object as a file. 

R uses "<-" to assign a value (or another object) to an object. You may find "=" means the same, but we don't recommend it, because you also use "=" with different meaning in the command and parameter.

```{r objects}
# assign
x <- 5
y <- 2
```

You can call up what is stored in the object (inspect) again by just typing its name:  

```{r inspectObj}
x
```

These objects will show up in the "Environment" window in  Rstudio, or you can use `ls()` in the console to list the objects. The function `c()` can be used to stick things together into a vector. Redo the below commands in your own script.  

```{r objects_in_R}
# a vector
x = c(1,2,5,7,8,15,3,12,11,19)
# another vector
y = 1:10
# you have now two objects
ls()
# you can add, multiply or subtract
z = x + y
z
zz = x * y
zz
zzz = x - y
zzz
foo = 0.5*x^2 - 3*x + 2
foo
```

## A dataframe  

A dataframe is a bit more complex, and here is a simple demonstration of its power.

```{r dataframe}
Rainfall <- data.frame(City = c("Montevideo","New York", "Amsterdam","Sydney", "Moscow", "Hong Kong"),
Rain_mm = c(950, 1174, 838, 1215, 707, 2400))
Rainfall
```

As you can see a data.frame can mix character columns (City) and numeric columns (Rain_mm). Here I used `c()` to generate vectors which I put in the columns. In addition, the colummns have names, which you can access using `colnames()`:
`r colnames(Rainfall)`  

Once you have a dataframe, you can access parts of the dataframe or manipulate the dataframe.

```{r workdf}
# call a column
Rainfall$City
# or
Rainfall["City"]
# or
Rainfall[,1]

# find a row
Rainfall[Rainfall["City"]=="Montevideo"]

# see the first two rows
Rainfall[1:2,]

# find a subset
lots <- Rainfall[Rainfall["Rain_mm"] > 1000,]
lots
```

### Exercise

Using the above examples, can you do the following?  

- Extract the column with the rainfall values?  
- Extract the row with the annual rainfall at Amsterdam?  
- Which cities have rainfall below 1500 mm?  


## The working directory  

Generally R works from a "working directory". This is the directory on disk where it expects to find files or write files to. You can set this in Rstudio via the menu item "Session" --> "Set working directory", but you can also set this in code. Setting the working directory is useful when you want to access data in files on your computer or the network.

The basic function to use is `setwd("path/to/file")`. The thing to note is that in the path description you have to use "forward /" rather than the standard windows "backward". 

```{r workingdirectory}
# set the working directory
setwd("~/Documents/2018/SSEAC_2017/opendata_workshop/Introduction_to_R")

# see some of the files
dir()[1:10]
```

```{r reset_workdir,echo=F}
#setwd(paste(root,"UruguaySWAT/UruguayCourse",sep="/"))
```

## Reading data from different sources

There are a multitude of functions to read data from the disk into the R memory, I will demonstrate only a few here.

Because a lot of data is stored in comma delimited txt files (such as Excel exports), using `read.csv()` is a good standard option. 

Here I am reading in some monthly data from the Concordia station in the Uruguay river in Argentina. This data was originally downloaded from the 
[Global River Discharge Database]("https://nelson.wisc.edu/sage/data-and-models/riverdata/index.php")  

```{r read_csv}
UR_flow <- read.csv("UruguayRiver_ConcordiaSt.csv")
# check the first few lines (6 by default)
head(UR_flow)
```

Previously you would have to save a binary data file, say in `xls` in to `csv` or `txt`. As R progresses, now you could load a dataset directly from its binary format. There are many packages to do such task, [readxl package](https://cran.r-project.org/web/packages/readxl/readxl.pdf) is one of them. You could google your way of the most convenient package to use.


### Exercise  

- Can you read in the file: "Parana_CorrientesSt.csv"?

# STATISTICAL ANALYSIS AND DATA MANIPULATION

Now it's time to dig ourselves in to a more technical bits. How to manipulate data so we can perform some analyses on it to answer our research problem. There are, offcourse, base R commands to do the job, but find it easier for us to use `tidyverse` package. This package is actually a combo of several packages written by the same author.

## Packages to use

Much of the power in R comes from the fact that it is open source and this means many people write new code and share this code. The formal way to do this is via "packages", which, once checked and endorsed by the R community, appear in the CRAN repository as a **package**.

Here we might want to use some of the features in the package [tidyverse](http://tidyverse.org/). The other package we will use later is the package [zoo](https://cran.r-project.org/web/packages/zoo/index.html)

There are two components to using packages. The first is to make sure that the package is installed, for which we can use the function `install.packages()`. Note that the name of the package is a *string* so needs to between quotes `""`.  

```
#install.packages("tidyverse")
```

If the package is installed in your personal library, you will need to load the package in R using `require()` or `library()`. There are subtle differences between these two functions, but they are currently not that important. Check the help files.

```
require(tidyverse)
```

### Exercise  

- Can you load the package zoo? 

## Statistical analysis

### Summarising data  

It is often important to summarise data, for example we might want to know the average monthly flow or the standard deviation of flow. R of course have several functions to deal with this. 

### Standard statistical functions

Here are some simple examples of standard statistical functions `mean`, `sd` and `cor` (and of course there are many more).

```{r statfun}
# average monthly flow
mean(UR_flow$Flow)
# st dev average flow
sd(UR_flow$Flow)

# subset two years and correlate
flow1969 <- UR_flow[UR_flow$Year==1969,]
flow1970 <- UR_flow[UR_flow$Year==1970,]

cor(flow1969$Flow,flow1970$Flow)
```

### Using aggregate()  
Another useful function is `aggregate()`, which allows you to apply a function over data frame and particular across different factors. Here is an example of summing the Uruguay river flow by year.

```{r aggregate_demo}
# aggregate to annual flow
(annual_flow <- aggregate(UR_flow,list(Year=UR_flow$Year),sum))

```

Note that the parentheses around the statement means that the result of the statement is printed. 

#### Exercise  

- Can you calculate the standard deviation of the monthly flow by year?

## Data manipulation (using `tidyverse`)

Make sure you've done this.

```
install.packages("tidyverse")
library(tidyverse)
```

## Important commands

The following list is the important commands to remember:

- `select()` 	select columns
- `filter()` 	filter rows
- `arrange()` re-order or arrange rows
- `mutate()` 	create new columns
- `summarise()` 	summarise values
- `group_by()` 	allows for group operations in the “split-apply-combine” concept

Let's open this dataset. It's a water quality data in `csv` format.

```{r chemdata}
chemdata <- read.csv("semarang_chem.csv", header = TRUE)
head(chemdata)
```

### `select()`

Suppose you want certain columns for your analysis. Use `select()`. In `tidyverse` package, we could use pipe operator `%>%` to give series of command. Instead of using many brackets. 

```
chemdata %>%
  select(Lat, Long) %>%
  View("chemdata")
```

Or you want multiple columns `Lat`, `Long` until `Depth`. Use `select()` function.

```
chemdata %>%
  select(Lat, Long:Depth) %>%
  View("chemdata")
```

Or you want multiple columns Lat, Long until Depth, but you don't want UTM_zone. Use `select()` function.

```
chemdata %>%
  select(Lat, Long:Depth, -UTM_zone) %>%
  View("chemdata")
```

### `filter()`

You want to select all data from Damar Formation. Use `filter()` function.

```
chemdata %>%
  filter(Aq == "Damar") %>%
  View("chemdata")
```

### `arrange()`

Sorting out data by Aq and Fac. Use `arrange()` function.

```
chemdata %>%
  arrange(Aq, Fac) %>%
  View("chemdata")
```

### `mutate()`

Making new columns, for instance, calculating the ration between Ca and Na. Use `mutate() function`

```
chemdata %>% 
  mutate(ratio_Cana = Ca / Na) %>%
  View("chemdata")
```

### `summarise()`

Making a summary from your data. Use `summarise()` function.

```
chemdata %>% 
  summarise(mean_TDS = mean(TDS), 
            max_Cl = max(Cl),
            min_Cl = min(Cl),
            total = n())
```

### `group_by()`

Sorting out the data based on certain order. Use `group_by()` function.

```
chemdata%>% 
  group_by(Aq) %>%
  summarise(mean_TDS = mean(TDS), 
            max_Cl = max(Cl),
            min_Cl = min(Cl),
            total = n())
```

#### Exercise

- try to evaluate your data by `group_by()`
- try to summarise your own data using `summarise()`


#PLOTTING

R is good at plotting. There are many ways to create a plot. So you just have to choose which one is the easiest for you. One way is using base R plotting engine. Like these plots.

```{r plotting1}
hist(chemdata$SO4)
hist(chemdata$EC)
hist(chemdata$TDS)
plot(chemdata$TDS, chemdata$Cl)
```

Maybe you want to look at them in one panel.

```{r plotting2}
par(mfrow=c(2,2))
hist(chemdata$SO4)
hist(chemdata$EC)
hist(chemdata$TDS)
plot(chemdata$TDS, chemdata$Cl)
```

You could always tweak the plot to suits your needs. There are many resources about plotting in R, like: 

- [Producing Simple Graphs with R](https://www.harding.edu/fmccown/r/), 
- [Quick R](https://www.statmethods.net/graphs/).
- and more.

Or you could you `ggplot2` plotting engine from `tidyverse`.

```{r}
library(ggplot2)
```

```{r plotting3}
ggplot(chemdata, aes(EC, fill = Fac)) + 
  geom_histogram()
ggplot(chemdata, aes(Cl, TDS, colour = Fac)) + 
  geom_point()
```

## Exercise

- try to make a plot between Ca vs Na using base R and ggplot2.
- try to make histogram for one parameter that you have in your dataset. Use base R and ggplot2.
- can you tweak it by adding title to the plot and title to all axis.

# BASIC STATISTICAL ANALYSIS

## Summarising data  
It is often important to summarise data, for example we might want to know the average monthly flow or the standard deviation of flow. R of course have several functions to deal with this. 

```{r read_csv}
UR_flow <- read.csv("UruguayRiver_ConcordiaSt.csv")
# check the first few lines (6 by default)
head(UR_flow)
```

## Standard statistical functions
Here are some simple examples of standard statistical functions `mean`, `sd` and `cor` (and of course there are many more).

```{r statfun}
# average monthly flow
mean(UR_flow$Flow)
# st dev average flow
sd(UR_flow$Flow)

# subset two years and correlate
flow1969 <- UR_flow[UR_flow$Year==1969,]
flow1970 <- UR_flow[UR_flow$Year==1970,]

cor(flow1969$Flow,flow1970$Flow)
```

### using aggregate()  
Another useful function is `aggregate()`, which allows you to apply a function over data frame and particular across different factors. Here is an example of summing the Uruguay river flow by year.

```{r aggregate_demo}
# aggregate to annual flow
(annual_flow <- aggregate(UR_flow,list(Year=UR_flow$Year),sum))

```

Note that the parentheses around the statement means that the result of the statement is printed. 

#### Exercise  

- Can you calculate the standard deviation of the monthly flow by year?

## if else and for loops

Loops are essential in programming. There are a range of different types of loops, but here I will only demonstrate "if else" and "for". I have always been told that all other loops are just derivatives or short cuts.  
An "if else" loop allows you to program a switch in the code.  
Basically it is used to evaluate an expression if the statement is TRUE and to evaluate another expression if the statement is FALSE:  
if (comparison) do this, else do that  
You might call the above line "pseudo code". It is sometimes handy to first write something in pseudo code, basically you want to write in broad language what you want to happen.  
You could also use only the "if" part, which means nothing happens if the statement is FALSE.  

As an example I want to do:
if (a data frame has more than 10 rows) write data frame is LONG, else write data frame is SHORT.

```{r ifelseLoop}
#We will first generate the data frame:
x <- UR_flow
# now wrote loop
if (nrow(x) > 10) {
print("the dataframe is LONG")
} else {
print("the dataframe is SHORT")
}
```

There are a few things to note here. I use the statement `nrow()` to check how many rows the data frame has. I use the statement `print()` to write something to the screen.

### Exercise  

- Try generating another data frame x and rerun the program, or change the program to get it to say "the dataframe is short".  

R also has an `ifelse()` command. This is a vectorized version of the if command - which means that it can be used on vectors of data - the command is applied to each element (or value) of the vector in turn.  The if command only evaluates single values.  
Using the ifelse command will return a vector of values, the same length as the longest argument in the expression.  
Wherever possible, it is preferable to use the ifelse command rather than using the if command in combination with a loop - writing the program is more efficient and R evaluates vectorised functions more efficiently than it does loops. Here is an example which changes the program above.

```{r vectorisedIfelse}
x <- UR_flow 
# add a column which identifies whether the flow < 5000
x[,4] <- ifelse(x[,3] > 5000, "large", "small")
# this creates a third column
tail(x,10)
```

I check in the second column of the data.frame whether the flows are greater than 5000 or not. I then write in the third column whether they are large or small numbers. 
A more complex (nested) ifelse version would be:  

```{r nestedifelse}
x[,5] <- ifelse(x[,3] > 2500,ifelse(x[,3] > 10000, "large", "intermediate"), "small")
tail(x,10)
```

You can try out some of your own versions of this  

## The "for" loop, getting the program to do something repeatedly
Loops are used to repeat a set of commands.  Normally, there will be a variable which changes value in each successive loop through the commands.  Reference to this changing value results in differences in output from successive iterations. 

The for loop is used when the number of required iterations is known before the loop begins.
It is used in the following way:
for (name in expression1) {expression2}

* name is the name of the loop variable.  Its value changes during each iteration, starting with the first value and ending with the last value in expression1.
* expression1 is a vector expression (often a sequence, such as 1:10).
* expression2 is a command or group of commands that are repeatedly evaluated.  It usually contains references to name, which result in changes to the value of the expression as the value of name changes.  

Here is the classic example of a loop  

```{r helloworld}
# Hello world
for (i in 1:5) {
print(paste(i, "hello world"))
}
```
Note the use of `paste()` to combine character vectors.  
Here is another simple loop that tells you the first 5 values of the flow data.  
```{r simple_loop}
for (i in 1:5) {
print(paste(UR_flow$Flow[i], "is the flow (ML/day)"))
}

# or more complex:
for (i in 1:5) {
print(paste("in Year", UR_flow$Year[i],"and month", 
UR_flow$Month[i], 
"the flow is", UR_flow$Flow[i], "(ML/day)"))
}

```

You can also nest loops, that is, embed one loop into another. Here is an example that prints both the year and the flow using the column names  in the dataframe.  
```{r nested_loops}
for (i in 1:5) {
for (j in c(1,3)) {
print(paste(UR_flow[i,j], colnames(UR_flow)[j]))
}
}

```

### Exercise    

- Write another program that includes a loop and a logical test  

###Comparison and Logical Operators

*Comparison operators return a true or false value:*  

* ==	Equal to
* >	Greater than
* >=	Greater than or equal to 
* <	Less than
* <=	Less than or equal to  

Comparison operators can be combined with logical operators to describe more complex conditions.  

*Logical operators:*

* !	Not
* |	or (used for vectors, with the ifelse command)
* ||	or (used for single values)
* &	and (used for vectors, with the ifelse command)
* &&	and (used for single values)  


### Exercise  

Write a small program that uses comparison operators and a logical operator. 
# Plotting using ggplot and using zoo

R has many different plotting options, but recently ggplot2 appears to be the preferred option. One of the reasons for the support for ggplot2 is because it can also be used in Python.

As a start I will first modify the UR_flow data to make this into a "zoo" data frame, which has a timeseries based index and helps with plotting.

The function basically takes the data and links it to an index (such as the date). I first construct a vector of dates using the month and year from `UR_flow`.  

```{r convertToZoo}
#install.packages("zoo")  do this if you haven't done so.
require(zoo)
UR_flow$Dates <- as.Date(paste(UR_flow$Year,UR_flow$Month,"01",sep="-"))
UR_flow_z <- zoo(UR_flow$Flow, order.by = UR_flow$Dates)
```

Once we have this zoo data frame, it can be plotted quite easily with the basic plotting package.  

```{r simplePlot, fig.cap="Demonstration of the simple plotting package in R"}
plot(UR_flow_z)
```

Using ggplot2 is a bit more involved, and does not work well directly with zoo. So we need to go back to UR_flow.

```{r zooPlot, fig.cap="Demonstration of the ggplot2 package in R"}
p <- ggplot(UR_flow, aes(x=Dates, y = Flow)) + geom_line()
print(p)
```

# Writing functions in R

Until now you have used several functions in R that are part of packages or part of "core" R. However, another powerful element in R is the ability to write your own functions. There are two major advantages with writing functions:  

1. They are easy to test, as they are contained. This is especially true if keep functions short.
2. They are short cuts and repeatable and therefore limit the possibility of typos.

Let's go back to the "hello world" example that we used in a loop earlier. We can write the same example in a function.  
The first thing to do is to decide which inputs we want the function to use to create the output. In this case I suggest we might want to change how many times the function produces output (which was 5 in the earlier example) and the actual output text, which was "hello world" in the original function.

The basic structure of a function is:  
  
  `NameOfFunction <- function(input1, input2,...) {`    
`doSomething <- ....`  
`return(doSomething)`  
`}`

Here is the hello world example:
  ```{r helloFun}

HW <- function(n, outtext) {
  for (i in 1:n) {
    print(outtext)
  }
  # return("nothing")
}

# test
HW(5, "Hello World")
# switch input by naming
HW(outtext ="I can switch the inputs", n = 3)
```

Note that in this case the function produces output as part of its execution rather than returning an actual value (which is why I commented out the `return` statement). In the first example, you can see that you don't have to name the inputs if you keep the inputs in the same order as the defined function. R assumes that you mean `n = 5` and `outtext = "Hello world"`. In the second example I show that you can switch the inpurs if you name them and that the function allows you to choose different inputs.  

### Exercise

- Can you write a function that calculates y = a*x + b for different values of x, a and b?
- Make the function return the output using return()

# R for hydrological modelling (hydromad example)

The below example is a bit more complex but demonstrates the power of packages, and how you can use R to do hydrological modelling using the package hydromad.

The R package Hydromad is an add-in into R that deals specifically with hydrological modelling: <http://hydromad.catchment.org/>. It incorporates a range of models, and provides tools for calibration, validation and error checking.
Installation of Hydromad can be a bit tricky, ask Willem for help if you want to install it on your personal computer:

* In R, install a series of packages: 
'install.packages(c("zoo", "latticeExtra", "polynom", "car", "Hmisc"))'  
* Follow the instructions on: <http://hydromad.catchment.org/#installation> to install the main package  

On difference in the plotting is that hydromad uses the package lattice rather than ggplot2 (which I demonstrated earlier). Maybe in the future this can be changed to ggplot2, but currently this is not a priority.
This means that you will see the functions `xyplot()` and `levelplot` used.

Once the package is installed, load it:
```{r hydromad}
#install.packages("hydromad") delete "#" if you have done it
require(hydromad)
```

And we will use the data from the Cotter river that come with the package
```{r loadCotter, fig.cap = "plot of the data in the Cotter data set"}
data(Cotter)
xyplot(Cotter)
```
Note, if you can't install `hydromad` package from CRAN using base command, try to install it directly from GitHub using the following commands. You must install `devtools` package first.

```
library(devtools)
install_github("josephguillaume/hydromad") 
```
The "josephguillaume/hydromad" is the repository name to hold the `hydromad` source code. You could run this workaround if you have trouble installing package from CRAN repository.

## GR4J

GR4J is a simple conceptual rainfall-runoff model, developed in France. The associated paper is:
Perrin, C., Michel, C., Andr?assian, V., 2003. Improvement of a parsimonious model for streamflow simulation. Journal of Hydrology, 279(1-4): 275-289.

The power of GR4J is that it only has 4 parameters, which you could also consider its limitation. However, the strategic choices within the structure means that the model is highly flexible and easy to fit to all kinds of streamflow data.

```{r GR4J_graphic, echo=F}
include_graphics("../../satellitecalibration/data/GR4JStructure.png")
```

## Defining the model  

We now need to specify our first model. The beauty of hydromad is that there actually a range of different models that can be applied and these are discussed in much more detail in Andrews et al. (2011). We will just go through the specification of GR4j. The first model we will define is purely for illustration and is actually never fitted.  

Note how in the hydromad function the soil moisture accounting (the water balance) component is defined by `sma` which in this case is `gr4j` and the routing component is defined separately as `routing = "gr4jrouting"` .  
You could include different `sma` and different `routing`.
The etmult is a multiplier to transform maximum temperature into potential ET (PET). This is because maximum daily temperature and PET are highly correlated with a slope of approximately 0.15.  

```{r simplGR4JDef, fig.cap="Plot of GR4J with hypothetical parameters"}
# specify a model structure (GR4J) with some arbritrary parameters
CMod <- hydromad(Cotter[1:1000,], sma="gr4j", routing="gr4jrouting",etmult=0.15,
x1 = 665, x2 = 10, x3 = 90, x4 = 3.8, 
S_0 = 0.6, R_0 = 0.7)
# in other words, this model is purely arbitrary to demonstrate 
# the different components
xyplot(predict(CMod, return_state = TRUE, return_components = TRUE),
strip = FALSE, strip.left = TRUE)
# plot this to show the components
```

This figure indicates the different components of the GR4J model. This includes the "effective rainfall" U, which is the rainfall minus the ET, the storage levels in the "production store" or soil bucket, the predicted ET from the soil bucket, the fast flow Xr and the slow flow Xd and finally the levels in the groundwater store.  
Note the definition of the parameters x1, x2, x3 and x4 which relate to processes in the model, and it might be worth considering how these influence the model.

**You can try changing the parameters and see if it changes the model.**  
We can also ask for the parameters and the information that we have stored in the model object:  
```{r printPar}
print(CMod)
```

The first part of the output from the print statement indicates the SMA parameters that we have entered in the model. Here x1 is set to 665 mm and again the et multiplier is set to 0.15 to convert Maximum Temperature to potential ET. S_0 is just the initial storage level as a fraction of x1, this means the initial S is 0.6*665, you can check this on the figure. The default ranges for all parameters for all models can be found by entering: `hydromad.options()`

The routing parameters relate to the x2 through x4 parameters and are listed in the second part of the output.
```{r printCoef}
coefficients(CMod)
```

## optimisation using hydromad  
We now want to calibrate the model on some data, we will just take the first part of the data so we will use about 7 years

```{r}
# split the data to use for calibration 
Data_Cal<- window(Cotter, start = "1970-01-01",end = "1975-12-31")
```

You can now start fitting the model, but this first means we have to give the model some ranges to fit for the parameters rather than single values. I have decided to fit all parameters. The choice of the ranges is somewhat arbitrary, but I have increased the range for x1 to be quite high.

```{r}
CMod <- hydromad(Data_Cal, sma="gr4j", routing="gr4jrouting",
etmult=c(0.05,0.5),x1 = c(1000,3000), x2 = c(-3,20), 
x3 =c(5,500), x4 = c(0.5,10), S_0 = 0.5, R_0 = 0.5)
```

Then using the function `fitByOptim` we can calibrate the model
```{r}
CotterFit <- fitByOptim(CMod,objective=~hmadstat("r.squared")(Q,X),
samples=1000,method="PORT")
```

This fits the model using the "optim" routine in R to optimise the parameters in the model (see `?optim`).
`optim()` is quite a complex function, but very useful for all types of optimisation problems. 
The method `PORT` is one of the specific methods that can be used, but different other methods can be specified. Felix Andrews  (Andrews et al. 2011) suggests that this routine is preferred, but you could try another method. The standard `nelder mead` is a regular least squares optimisation.

The fitting gives some output about the iterations it runs through and also possible warnings. You can ask for some information about the model:

```{r}
summary(CotterFit)
```

This returns the models fitted values and the different goodness of fit stats. Note that `r round(summary(CotterFit)$r.squared*100,2)` percent of the variation in the streamflow is being explained by this model based on the r^2^, which is really the NSE (Nash Sutcliffe Efficiency). 

The other statistics are the relative bias (basically the mean error divided by the mean flow) which is about `r summary(CotterFit)$rel.bias`. The r.sq.sqrt is the r^2^ based on a square root transformation of the data, while the r.sq.log is the r^2^ based on a logarithmic transformation of the data. It is suggested that these two statistics are more indicative of the fit on the low flows, while the r^2^ is more indicative of the fit of the flow peaks (Bennett et al., 2013). So in this case the model seems to fit the low flows better than the high flows.  

The fitted coefficients can be extracted separately by asking:  
```{r}
coef(CotterFit)
```

Now we need to check how good our fit actually is. We can plot the observed data with the predicted output and the rainfall to see how it looks visually, but this is just showing how we fitted the data.  
```{r, fig.cap="Predicted and observed flow for Cotter for the calibration period"}
# plot observed vs modelled with the rainfall (Figure 5)
xyplot(CotterFit, with.P=TRUE, xlim=as.Date(c("1970-01-01", "1975-01-01")))
```

\newpage
\center
**END OF DOCUMENT**
\center



